---
title: "Introduction into machine learning and analysis of Breast Cancer Proteomes"
author: "Mats Slik"
email: "m.p.slik@st.hanze.nl"

output:
  pdf_document:
    fig_caption: true

header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
 \floatplacement{table}{H}
---

<!-- (Front page) -->
\vspace{350pt}

\hfill \textbf{Student}: Mats Slik

\hfill \textbf{Student number}: 344216

\hfill \textbf{Class}: BFV3

\hfill \textbf{Study}: Bioinformatics

\hfill \textbf{Institute}: Institute for Life Science & Technology

\hfill \textbf{Teachers}: Dave Langers (LADR) and Bart Barnard (BABA)

\hfill \textbf{Date}: `r Sys.Date()`

\newpage
<!-- (Table of contents) -->
\setcounter{secnumdepth}{2}
\tableofcontents
\pagenumbering{arabic}

```{r setup, include = 0}

# packages
library(pander)
library(tidyr)
library(dplyr)
library(ggplot2)
library(ggExtra)
library(ggpubr)
library(stringr)
library(RWeka)
library(MatrixGenerics)



knitr::opts_knit$set(root.dir = here::here())

```

\newpage
# Dataset



## About Dataset

### information about the data set and the three given files :
**Context:**
This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.
this data was sampled from 105 originally from the TCGA (The Cancer Genome Atlas Program - NCI), which was further filterd to 77 samples containing high quality protein expression data.
**Content:**

* **File:** 77cancerproteomesCPTACitraq.csv

  + **RefSeqaccessionnumber:** RefSeq protein ID (each protein has a unique
  ID in a RefSeq database)
  + **gene_symbol:** a symbol unique to each gene (every protein is encoded
  by some gene)
  + **gene_name:** a full name of that gene
  + **Remaining columns:** log2 iTRAQ ratios for each sample (protein
  expression data, most important), three last columns are from healthy
  individuals


* **File:** clinicaldatabreast_cancer.csv

  + **First column** "Complete TCGA ID" is used to match the sample IDs in the main cancer proteomes file (see example script).
  + **All other columns** have self-explanatory names, contain data about the cancer classification of a given sample using different methods. 'PAM50 mRNA' classification is being used in the example script.

* **File:** PAM50_proteins.csv

    * **Contains**  the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set.

**Past Research:**
Original research paper: https://www.researchgate.net/publication/303509927_Proteogenomics_connects_somatic_mutations_to_signaling_in_breast_cancer

**Summary:** the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, DNA repair, enzymatic reactions and signaling etc.

my question is: Are there different ways to categorize breast cancer based on protein expression data, with machine learning being used to classify them without using the pam50 proteins?


## Exploratory Data Analysis

loading of the dataframes and showing the successful loading and its dimensions.
note only the first 5 columns of "77_cancer_proteomes_CPTAC_itraq.csv" are shown since after column 4 they are the same type.
```{r data loading}

protein_exp_data <- read.csv(file = "Analysis//data//77_cancer_proteomes_CPTAC_itraq.csv")
clinical_data <- read.csv(file = "Analysis//data//clinical_data_breast_cancer.csv")
pam50_protein_data <- read.csv(file = "Analysis//data/PAM50_proteins.csv")

# showing succeseful loading of data

# only showing first 5 columns of proteomes
pander(head(protein_exp_data[1:5], n = 5))
pander(head(clinical_data, n=5))
pander(head(pam50_protein_data, n=5))

# showing the structure/dimensions of dataframe
cat("77_cancer_proteomes_CPTAC_itraq [ number of rows:", nrow(protein_exp_data), "number of columns:", ncol(protein_exp_data), '\n')
cat("clinical_data [ number of rows:", nrow(clinical_data), "number of columns:",ncol(clinical_data),'\n')
cat("pam50_protein_data [ number of rows:", nrow(pam50_protein_data), "number of columns:",ncol(pam50_protein_data),'\n')

```
Everything seems to be loaded completely, but we shall look further if everything is also correctly interpreted in R


Now checking if the protein expression data has been correctly read.
```{r Protein expression dataframe exploration}
str(protein_exp_data)
```
Nothing strange about the Proteomes dat everything seems to be read correct.

Checking if the clinical data  has been correctly read.
```{r clinical dataframe exploration}
str(clinical_data)
```
Nothing strange about the clinical data everything seems to be read correct.

Checking if the pam50 protein data  has been correctly read.
```{r pam 50 dataframe exploration}
str(pam50_protein_data)
```
Nothing strange about the pam50 protein data everything seems to be read correct.


### codebook

loading of the created codebooks for the three dataframes.
showing also its contents and successful loading
```{r code book visualisation}

cancer_proteomes_CPTAC_codebook <- read.csv2("Analysis//data/77_cancer_proteomes_CPTAC_codebook.txt")
clinical_data_codebook <- read.csv2("Analysis//data/clinical_data_breast_cancer_codebook.txt")
PAM50_protein_codebook <- read.csv2("Analysis//data/PAM50_protein_codebook.txt", sep = ";")


pander(cancer_proteomes_CPTAC_codebook)
pander(clinical_data_codebook)
pander(PAM50_protein_codebook)
```
Here we can also see that everything has been successfully loaded into R

## Data observation

There are 12553 rows in the data, these are proteins identifiable with a RefSeq ID number and have 86 columns of which the last 83 are samples(with their identifiers as there name and the last three from healthy individuals, but these shall not be used for the machine learning part since 3 samples is too little to use for analyzes.
to further use the data I shall reshape it to make the rows samples and each column a protein.

## Data cleaning and altering

### altering sample names

The alteration of sample names to corespondent to the clinical data names is needed for further comparison and analyses.
This is done by changing the column names to that of the same format of the clinical data.
This is done with some regex magic.
```{r sample name altering}
# storing a list of the column names
column_names <- names(protein_exp_data)

# function
change_sample_name <- function (x){
  #search for TCGA name,if found split and make new name
    if(grepl("TCGA",x) == TRUE){
      temp_list <- as.list(strsplit(x, '[_|-|.]')[[1]])
      x <- str_c(c('TCGA',temp_list[[1]],temp_list[[2]]),collapse = '-')
    }
return (x)
}

# changing of the colnames
colnames(protein_exp_data) <- lapply(column_names, change_sample_name)
cat("Old name:",column_names[[4]],",New name:",names(protein_exp_data)[[4]])

```
This output show to conversion has been successful

### numerical data frame
Now we need to make a data frame with only the numerical data for the ease of analyzes
```{r numerical dataframe}
# first making a data frame with only the numerical data, samples start at column number 4 til the end column number 86
protein_exp_numerical <- protein_exp_data[4:86]
```


### Transposing
Transposing the created data frame "protein_exp_numerical", and adding the refseq ID as column name
```{r Data transposing}

# transposing of the old dataframe to a new one
protein_exp__numerical_transposed <- as.data.frame(t(protein_exp_numerical))
colnames(protein_exp__numerical_transposed) <- protein_exp_data$RefSeq_accession_number

# checking if succesfull
cat("protein_exp_numerical number of rows:", nrow(protein_exp_numerical),
    "number of columns:", ncol(protein_exp_numerical), '\n')
cat("protein_exp__numerical_transposed number of rows:", nrow(protein_exp__numerical_transposed),
    "number of columns:", ncol(protein_exp__numerical_transposed), '\n')


```
As we can see the row and column dimensions have been flipped

### cleaning

Since there are NA values in the data lets see how much
```{r cleaning step for NA}
count_na_func <- function(x) sum(is.na(x))
# getting NA values per RefSeqID(column)
Na_per_col <- sapply(protein_exp__numerical_transposed, count_na_func)
```
```{r plot making1, fig.cap = "barplot with the frequency of columns with more than 0 NA in them", out.width = "90%"}

ggplot() +
  aes(Na_per_col) +
  geom_histogram(color = "black", fill = "#F9C000", binwidth = 3) +
  xlab("Number of NA") +
  ylab("Frequency of columns") +
  ggtitle("Frequency of number of NA values per RefSeqID")

ggsave(
  filename = "figure1.png",
  plot = last_plot(),
  path = "data/figures")

```
```{r plot making2 , fig.cap = "barplot with the frequency of columns with more than 0 NA in them", out.width = "90%"}
ggplot() +
  aes(Na_per_col[Na_per_col > 0]) +
  geom_histogram(color = "black", fill = "#0039F9",binwidth = 3) +
  xlab("Number of NA") +
  ylab("Frequency of columns") +
  ggtitle("Frequency of number of NA values per RefSeqID with 0 omitted")

ggsave(
  filename = "figure2.png",
  plot = last_plot(),
  path = "data/figures")

```
There are a lot of proteins with more than 10% of their samples with missing data, so i shall be removing.
```{r finishing NA cleaning}
cat("number of proteins with NA values in them:", sum(Na_per_col > 0), '\n' )

# deleting every protein with more than 10% NA values in them, since this is allot
proteomes_filtered_data <- protein_exp__numerical_transposed[Na_per_col < 8]
cat("number of proteins with 8 or more NA values in them and deleted from data:",
    sum(Na_per_col > 8), '\n')

cat("proteomes_filtered_data[number of rows:",
    nrow(proteomes_filtered_data),
    "number of columns:",
    ncol(proteomes_filtered_data),'\n')
```
As we can see from te reports generated by the code we can see that the filtering of NA was successful.
And we now have a data set that contains data with less than 10% per protein of NA values.

### Merging clinical and protein expression dataframes

To be able to use the Clinical data we need to merge it to its corresponding row and sample in the Protein expressions.
```{r data merging}
# firs assigning row names to clinical data
rownames(clinical_data) <- clinical_data$Complete.TCGA.ID


# removing the used ID column to simplifie it since it has been become redundend
clinical_data <- clinical_data[,-1]

# mergin the two data frames according to the row names (TCGA Identification number),
merged_data <- merge(select(clinical_data, Tumor, Tumor..T1.Coded, AJCC.Stage, Vital.Status), protein_exp__numerical_transposed, by = 0)
cleaned_merged_data <- merge(select(clinical_data, Tumor, Tumor..T1.Coded, AJCC.Stage, Vital.Status), proteomes_filtered_data, by = 0)
cat("merged_data of rows:", nrow(merged_data),
    "number of columns:",ncol(merged_data),'\n')
```
We can see that not every sample had an entry in the clinical data, so we end up with 6 rows of sample data that get left out of the merged data set.

## Data visualisation

Showing some examples of distributions of protein expression data since there are around 12 000 proteins found.
```{r plot making3, fig.cap = "boxplot of protein expression distribution, first 70 proteins", out.width = "90%"}

# Open a png file
#png("../data/figures/figure3.png")
# 2. Create a plot
boxplot(merged_data[6:76],
        col = rainbow(ncol(merged_data[6:76])),
                           xlab = "Protein",
                           ylab = "log2 iTRAQ ratio",
                           main = "distribution of Protein expression for first 70 Proteins",
                           show.names= FALSE)
# Close the pdf file
#dev.off()
```
Since I can't be sure of the significance of every protein I can't simply trow away any sample.

To fix this I shall get the standard deviation from every protein to prepare for a selection of the most deviant ones.
And for illustration I will overlay the dataframe which has filtered out the proteins with more than 10% of their values being NA.

```{r plot making4, fig.cap = "scatterplot of standard deviation for the protein expresion data and for the NA filterd version", out.width = "90%"}

fg <- colSds(as.matrix(merged_data[sapply(merged_data, is.numeric)]), na.rm = TRUE)
fg2 <- colSds(as.matrix(cleaned_merged_data[sapply(cleaned_merged_data, is.numeric)]), na.rm = TRUE)


fg <- as.data.frame(fg, row.names = colnames(merged_data[6:ncol(merged_data)]))
fg2 <- as.data.frame(fg2, row.names = colnames(cleaned_merged_data[6:ncol(cleaned_merged_data)]))

fg$count <- seq(from = 1, to = nrow(fg))

merged_fg <- merge(fg,fg2,all.x = TRUE, by = 0)
merged_fg <- merged_fg[order(merged_fg$count),]

# assigning 0 to every NA values in fg2 since that is the filterd one
merged_fg$fg2[is.na(merged_fg$fg2)] <- 0

# plotting
ggplot(data=merged_fg) +
  geom_point(size=0.0015, aes(x=count,  y=fg, color="Non Na filtered")) +
  geom_point(size=0.0015, aes(x=count,y=fg2,  color="Na filtered")) +
  labs(x = "Data Frame row number",
         y = "Standard deviation",
         color = "Legend") +
  ggtitle("Density plot for standard deviation of protein expression") +
  theme(legend.position = "bottom")

ggsave(
  filename = "figure4.png",
  plot = last_plot(),
  path = "data/figures")

```


As we can see in the standard deviation plot from the proteins it is visible that  the proteins with a lot of NA in them seem to be having a higher Standard deviation, but still there should be enough deviation in the filtered Data to use it for further analyse using machine learning.
```{r Assigning factors}
# assigning as factors
merged_data$Tumor <- factor(merged_data$Tumor)
cleaned_merged_data$Tumor <- factor(cleaned_merged_data$Tumor)
```
```{r plot figure , fig.cap = "distribution of amount of samples per tumor stage"}
plot(cleaned_merged_data$Tumor, ylab = "Count", xlab="tumor stage", col ="#F9C000", main = "distribution of tumor stage")
```
/newpage


# Supervised Learning

In this chapter, we are looking at how we are going to train the machine learning algorithms to accurately predict the tumor stage of breast cancer according to the protein expressions found in breast tissue samples.
After that there shall be an examination of the result and accuracy of the created models accordingly to different algorithms.

## Weka

Weka is used for the data examination and machine learning part, Weka is an open source collection of machine learning algorithms for data mining tasks. It contains tools for data preparation, classification, regression, clustering, association rules mining, and visualization. java platform for
Firstly the data is exported to a .arff file, so it can be loaded into Weka.


### data exportation
```{r}
train2 <- cleaned_merged_data[, -3:-5]
train3 <- train2[,-2]
train2$data.class <- as.factor(train2$Tumor)
```
```{r Data exportation}

write.arff(train3, file = "Analysis/data/train3.arff")
```
## Models

### experimenter
To firstly make a simple comparison for the effectiveness of the different algorithms we shall use the Weka experimenter.
The algorithms compared are as follows in the tabel beneath. note ZeroR is left out since it trows an error when running in the experimenter.

\begin{table}[thb]
\caption{\label{Weka Experimenter algorithms}Tabel with the algorithms with default settings used in initial comparison .}
\footnotesize
\begin{tabular}{l}
 (1) rules.OneR '-B 6' -3459427003147861500 \\
 (2) trees.RandomTree '-K 0 -M 1.0 -V 0.001 -S 38' -9051119597407395800 \\
 (3) trees.RandomForest '-P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 38' 1116839470751428740 \\
 (4) trees.J48 '-C 0.25 -M 2' -217733168393644448 \\
 (5) meta.AttributeSelectedClassifier '-E \"CfsSubsetEval -P 6 -E 6\"  -S \"GreedyStepwise -T -1.7976931348623157E308 -N -1 -num-slots 1\" -W trees.J48 -- -C 0.25 -M 2' -1151805453487947520 \\
 (6) meta.AttributeSelectedClassifier '-E \"CfsSubsetEval -P 6 -E 6\"  -S \"BestFirst -D 2 -N 5\" -W trees.J48 -- -C 0.25 -M 2' -1151805453487947520 \\
 (7) meta.AttributeSelectedClassifier '-E \"CfsSubsetEval -P 6 -E 6\"  -S \"BestFirst -D 2 -N 5\" -W trees.RandomForest -- -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1' -1151805453487947520 \\
 (8) meta.AttributeSelectedClassifier '-E \"CfsSubsetEval -P 6 -E 6\"  -S  \"BestFirst -D 2 -N 5\" -W trees.RandomTree -- -K 0 -M 1.0 -V 0.001 -S 38' -1151805453487947520
\end{tabular}
\end{table}

the results are as follows

\begin{table}[thb]
\caption{\label{Weka Experimenter Results}Tabel with a T test performed on the percentage each algoritme correctly predicted.}
\footnotesize
\begin{tabular}{l|llllllll}
 Dataset & (1) rules.On  & (2) trees & (3) trees & (4) trees & (5) meta. & (6) meta. & (7) meta. & (8) meta.  \\ \hline
 R data frame & 59.74 & 50.65 & 66.23 & 35.06 * & 45.45 & 42.86 * & 62.34 & 46.75 \\
 significance  & (v/ /*)  & (0/1/0) & (0/1/0) & (0/0/1) & (0/1/0) & (0/0/1) & (0/1/0) & (0/1/0)
\end{tabular}
\end{table}

##### Conclusion \newline

 As we can see from these results nothing really stands out from the rest, they all perform quit bad especially if u compare it with the OneR and ZeroR.
This is due to having a lot of attributes in my date with relatively a low number of instances, so overfitting is a major issue. This forces use to make some selection in attributes. this in turn forces us the meta.AttributeSelectedClassifier to first make a selection of the attributes and further refine these classifiers.

So for further testing we must firstly make a good baseline with the zeroR and further make comparisons with mutiple combinations of the meta.AttributeSelectedClassifier and its parameters. \newline

The next set of test where done in the weka explorer gui. \newline

The first of these is a ZeroR and the rest are the results of a few of the best AttributeSelectedClassifier, since most of them are extremely poor performing and not worthy of mentioning the results at all.
After showing these results we shall make a conclusion about which algorithm performs the best.
All of the following test runs have been made with crossvallidation with the leave one out method to maximise the limited number of instances in the data.
\newpage

### ZeroR \newline

The first algorithm used is a zeroR one is produced the following results. \newline


\begin{table}[tbh]
\caption{\label{Results ZeroR}Tabel with the summary of results from zeroR}
\footnotesize
\begin{tabular}{l|ll}
 Correctly Classified Instances & 51 & 66.2338 \\ \hline
 Incorrectly Classified Instances & 26 & 33.7662 \\ \hline
 Kappa statistic  & 0 & x \\ \hline
 Mean absolute error & 0.2665 & x \\ \hline
 Root mean squared error & 0.3613 & x \\ \hline
 Relative absolute error & 100 & x \\ \hline
 Root relative squared error & 100 & x \\ \hline
 Total Number of Instances & 77 & x
\end{tabular}
\end{table}


##### confusion matrix \newline


\begin{table}[tbh]
\caption{\label{ZeroR Confusion matrix}Confusion matrix}
\begin{tabular}{llll|l}
 a & b & c & d & <-- classified as \\ \hline
 0 & 10 & 0 & 0 & a = T1 \\ \hline
 0 & 51 & 0 & 0 & b = T2 \\ \hline
 0 & 11 & 0 & 0 & c = T3 \\ \hline
 0 & 5 & 0 & 0 & d = T4
\end{tabular}
\end{table}

##### algorithm conclusion \newline

As we can see from the results of ZeroR that even classifing everything as T2 scores 66% good, thus using that as a base of evaluation for the classifiers we used is not very reliable, and we shall take the confusion matrix and ROC curves more as an indication for a good algorithm to use for our data.
\newpage

### AttributeSelectedClassifier with cost sensitive J48 \newline

This is the second algorithm used, and this is using attribute selection with sub set evaluation based on the best first method.
As a cost matrix I assigned every wrongly classified instance as class T2 extra heavy since that class is overrepresented, and further weight that every clas that is wrongly classified a little heavier

Relation:     R_data_frame \newline
Instances:    77 \newline
Attributes:   9200 \newline
Test mode:    77-fold cross-validation \newline
Evaluation cost matrix: \newline

\begin{table}[tbh]
\begin{tabular}{llll}
 0 & 5 & 2 & 2 \\
 1 & 0 & 1 & 1 \\
 1 & 5 & 0 & 2 \\
 2 & 5 & 2 & 0 \\

\end{tabular}
\end{table}

=== Attribute Selection on all input data === \newline

Search Method: \newline
    Best first. \newline
    Start set: no attributes \newline
    Search direction: forward \newline
    Stale search after 5 node expansions \newline
    Total number of subsets evaluated: 211334 \newline
    Merit of the best subset found:    0.601 \newline

Attribute Subset Evaluator (supervised, Class (nominal): 9200 data.class): \newline
    CFS Subset Evaluator \newline
    Including locally predictive attributes \newline

Selected attributes: 338,905,1188,1230,1555,2172,2277,2821,3196,3333,3719,3932,5844,6802,7234,7490,7959,8149,8538 : 19 \newline
                     NP_008832
                     NP_056289
                     NP_001349
                     NP_055719
                     NP_001150
                     NP_079093
                     NP_000959
                     NP_004893
                     NP_004887
                     NP_065901
                     NP_058632
                     NP_002630
                     NP_060947
                     NP_065109
                     NP_848613
                     NP_001035147
                     NP_005639
                     NP_001092102
                     NP_001135757 \newline

\begin{table}[tbh]
\caption{\label{Results cost sensitive Results with attribute selector}Tabel with the summary of results from zeroR}
\footnotesize
\begin{tabular}{l|ll}
 Correctly Classified Instances & 22 & 28.5 \\ \hline
 Incorrectly Classified Instances & 55 & 71.4 \\ \hline
 Kappa statistic  & -0.1735 & x \\ \hline
 Total Cost &  135 \\ \hline
 Average Cost & 1.7532 \\ \hline
 Mean absolute error &  0.3473 & x \\ \hline
 Root mean squared error & 0.5733 & x \\ \hline
 Relative absolute error & 129.1465 & x \\ \hline
 Root relative squared error & 156.9896 & x \\ \hline
 Total Number of Instances & 77 & x
\end{tabular}
\end{table}




#### confusion matrix \newline

\begin{table}[tbh]
\caption{\label{cost sensitive J48 Confusion matrix}Confusion matrix}
\begin{tabular}{llll|l}
 a & b & c & d & <-- classified as \\ \hline
 1 & 7 & 2 & 0 & a = T1 \\ \hline
 8 & 19 & 19 & 5 & b = T2 \\ \hline
 2 & 8 & 1 & 0 & c = T3 \\ \hline
 0 & 4 & 0 & 1 & d = T4
\end{tabular}
\end{table}
\newpage

##### algorithm conclusion \newline

in the above seen confusion matrix we are looking for a nicely made line from the top left to the bottom right, but we can clearly see that that is not the case. So this algorithm doesn't have a lot of merit for further exploration


### AttributeSelectedClassifier HoeffdingTree
Relation:     R_data_frame \newline
Instances:    77 \newline
Attributes:   9200 \newline
Test mode:    77-fold cross-validation \newline
Evaluation cost matrix: \newline

\begin{table}[tbh]
\begin{tabular}{llll}
 0 & 5 & 2 & 2 \\
 1 & 0 & 1 & 1 \\
 1 & 5 & 0 & 2 \\
 2 & 5 & 2 & 0 \\

\end{tabular}
\end{table}

=== Attribute Selection on all input data === \newline

Search Method: \newline
    Best first. \newline
    Start set: no attributes \newline
    Search direction: bi-directional \newline
    Stale search after 5 node expansions \newline
    Total number of subsets evaluated: 248373 \newline
    Merit of best subset found:    0.604 \newline

Attribute Subset Evaluator (supervised, Class (nominal): 9200 data.class): \newline
    CFS Subset Evaluator \newline
    Including locally predictive attributes \newline

Selected attributes: 338,1188,1230,1555,2172,2277,2844,3196,3333,3719,3932,5844,6802,7234,7490,7959,8149,8538 : 18 \newline
                     NP_008832
                     NP_001349
                     NP_055719
                     NP_001150
                     NP_079093
                     NP_000959
                     NP_877496
                     NP_004887
                     NP_065901
                     NP_058632
                     NP_002630
                     NP_060947
                     NP_065109
                     NP_848613
                     NP_001035147
                     NP_005639
                     NP_001092102
                     NP_001135757 \newline

\begin{table}[tbh]
\caption{\label{Results attribute selector with HoeffdingTree}Tabel with the summary of results from HoeffdingTree}
\footnotesize
\begin{tabular}{l|ll}
 Correctly Classified Instances & 45 & 58.4 \\ \hline
 Incorrectly Classified Instances & 32 & 41.5 \\ \hline
 Kappa statistic  & -0.0584 & x \\ \hline
 Mean absolute error &  0.2337 & x \\ \hline
 Root mean squared error & 0.4332 & x \\ \hline
 Relative absolute error & 86.9241 & x \\ \hline
 Root relative squared error & 118.6126 & x \\ \hline
 Total Number of Instances & 77 & x
\end{tabular}
\end{table}




#### confusion matrix \newline

\begin{table}[tbh]
\caption{label{HoeffdingTree confusionmatrix}Confusion matrix}
\begin{tabular}{llll|l}
 a & b & c & d & <-- classified as \\ \hline
 0 & 9 & 1 & 0 & a = T1 \\ \hline
 5 & 45 & 1 & 0 & b = T2 \\ \hline
 1 & 10 & 0 & 0 & c = T3 \\ \hline
 0 & 5 & 0 & 0 & d = T4
\end{tabular}
\end{table}
\newpage

##### algorithm conclusion \newline

in the above seen confusion matrix we are looking for a nicely made line from the top left to the bottom right, but we can clearly see that that is not the case, and it classifies them mostly as t1 and t2. So this algorithm doesn't have a lot of merit for further exploration

### AttributeSelectedClassifier Ranker RandomTree
Relation:     R_data_frame \newline
Instances:    77 \newline
Attributes:   9200 \newline
Test mode:    77-fold cross-validation \newline
Evaluation cost matrix: \newline

\begin{table}[tbh]
\begin{tabular}{llll}
 0 & 5 & 2 & 2 \\
 1 & 0 & 1 & 1 \\
 1 & 5 & 0 & 2 \\
 2 & 5 & 2 & 0

\end{tabular}
\end{table}

=== Attribute Selection on all input data === \newline

Search Method: \newline
    Attribute ranking. \newline

Attribute Evaluator (supervised, Class (nominal): 9200 data.class): \newline
    Gain Ratio feature evaluator \newline

Ranked attributes: \newline
 0.735       4526 NP_071896 \newline
 0.735       6015 NP_004840 \newline
 0.735       1443 NP_006786 \newline
 0.735       1312 NP_002829 \newline
 0.65        8523 NP_002351 \newline
 0.532       2172 NP_079093 \newline
 0.49        1230 NP_055719 \newline
 0.481       5384 NP_006346 \newline
 0.481       2277 NP_000959 \newline
 0.481       4206 NP_000960 \newline
 0.481       8329 NP_000981 \newline
 0.439       1188 NP_001349 \newline

Selected attributes: 4526,6015,1443,1312,8523,2172,1230,5384,2277,4206,8329,1188 : 12 \newline


Header of reduced data: \newline
@relation 'R_data_frame-weka.filters.unsupervised.attribute.Remove-V-R4526,6015,1443,1312,8523,2172,1230,5384,2277,4206,8329,1188,9200' \newline

@attribute NP_071896 numeric \newline
@attribute NP_004840 numeric \newline
@attribute NP_006786 numeric \newline
@attribute NP_002829 numeric \newline
@attribute NP_002351 numeric \newline
@attribute NP_079093 numeric \newline
@attribute NP_055719 numeric \newline
@attribute NP_006346 numeric \newline
@attribute NP_000959 numeric \newline
@attribute NP_000960 numeric \newline
@attribute NP_000981 numeric \newline
@attribute NP_001349 numeric \newline


\begin{table}[tbh]
\caption{\label{Results of attribute selector with ranker }Tabel with the summary of results from Randomtree }
\footnotesize
\begin{tabular}{l|ll}
 Correctly Classified Instances & 42 & 54.5 \\ \hline
 Incorrectly Classified Instances & 35 & 45.5 \\ \hline
 Kappa statistic  & 0.1126 & x \\ \hline
 Mean absolute error &  0.226 & x \\ \hline
 Root mean squared error & 0.4726 & x \\ \hline
 Relative absolute error & 84.0 & x \\ \hline
 Root relative squared error & 129.4 & x \\ \hline
 Total Number of Instances & 77 & x
\end{tabular}
\end{table}




#### confusion matrix

\begin{table}[tbh]
\caption{label{RandomTree confusionmatrix}Confusion matrix}
\begin{tabular}{llll|l}
 a & b & c & d & <-- classified as \\ \hline
 3 & 4 & 3 & 0 & a = T1 \\ \hline
 3 & 38 & 7 & 3 & b = T2 \\ \hline
 5 & 6 & 0 & 0 & c = T3 \\ \hline
 0 & 4 & 0 & 1 & d = T4
\end{tabular}
\end{table}
\newpage

##### algorithm conclusion \newline

in the above seen confusion matrix we are looking for a nicely made line from the top left to the bottom right, but we can clearly see that that is not the case, and it classifies them mostly wrong as t2. So this algorithm doesn't have a lot of merit for further exploration

### AttributeSelectedClassifier greedystepwise with OneR
Relation:     R_data_frame \newline
Instances:    77 \newline
Attributes:   9200 \newline
Test mode:    77-fold cross-validation \newline
Evaluation cost matrix: \newline



=== Attribute Selection on all input data === \newline

Search Method: \newline
    Greedy Stepwise (forwards). \newline
    Start set: no attributes \newline
    Merit of best subset found:    0.601 \newline

Attribute Subset Evaluator (supervised, Class (nominal): 9200 data.class): \newline
    CFS Subset Evaluator \newline
    Including locally predictive attributes \newline

Selected attributes: 338,905,1188,1230,1555,2172,2277,2821,3196,3333,3719,3932,5844,6802,7234,7490,7959,8149,8539 : 19 \newline
                     NP_008832
                     NP_056289
                     NP_001349
                     NP_055719
                     NP_001150
                     NP_079093
                     NP_000959
                     NP_004893
                     NP_004887
                     NP_065901
                     NP_058632
                     NP_002630
                     NP_060947
                     NP_065109
                     NP_848613
                     NP_001035147
                     NP_005639
                     NP_001092102
                     NP_001017 \newline

\begin{table}[tbh]
\caption{\label{OneR results}Tabel with the summary of results from OneR}
\footnotesize
\begin{tabular}{l|ll}
 Correctly Classified Instances & 45 & 58.44 \\ \hline
 Incorrectly Classified Instances & 32 & 41.55 \\ \hline
 Kappa statistic  & -0.0788 & x \\ \hline
 Total Cost &  133   \\ \hline
 Average Cost &  1.7273 \\ \hline
 Mean absolute error &  0.2078 & x \\ \hline
 Root mean squared error & 0.4558 & x \\ \hline
 Relative absolute error & 77.2714 & x \\ \hline
 Root relative squared error & 124.8216 & x \\ \hline
 Total Number of Instances & 77 & x
\end{tabular}
\end{table}




#### confusion matrix

\begin{table}[tbh]
\caption{label{greedystepwise with OneR confusionmatrix}Confusion matrix}
\begin{tabular}{llll|l}
 a & b & c & d & <-- classified as \\ \hline
 0 & 9 & 1 & 0 & a = T1 \\ \hline
 2 & 45 & 4 & 0 & b = T2 \\ \hline
 0 & 11 & 0 & 0 & c = T3 \\ \hline
 0 & 5 & 0 & 0 & d = T4
\end{tabular}
\end{table}
\newpage

##### algorithm conclusion \newline

In the above seen confusion matrix we are looking for a nicely made line from the top left to the bottom right, but we can clearly see that that is not the case. We can see that is mostly classify them as T2. So this algorithm doesn't have a lot of merit for further exploration

## Supervised Learning Conclusion

#### summary
These are but the best of the multiple different settings that where tried, but all results where roughly the same as these presented in the chapters here above. One if not the first things that stands out for every one of these results is that none of these results scored an accuracy of correctly predicting the class of the data better than ZeroR with its 66.2%. One got close but that was a RandomTree model without attributeselection, so it is questionable how accurate its truly is since overfitting with 9200 attributes is a major problem. But the accuracy of correctly prediction the class is not everything, so we must also look at the confusion matrix's that where produced.
As can be seen from the confusion matrix's (tables 20,22,24,26) there are two trends visible that firstly there is a huge bias towards t2, this was tried to be compensated with different SMOTE functions to boost the underrepresented classes but added more than 75 % of synthetic data is not very accurate and introduces a whole host of new problems and bias to the algorithm used to boost the unrepresented classes. Furthermore, under sampling the T2 class also was not an option since cutting out instances in a data set only containing 77 is not good since this is already a very low sample size.
Thus, most of the models that where teste where with the meta learning AttributeSelectedClassifier , and yielded not much as explained above.
The main thing that can be said of all the methods and algorithms used is they performed all bad or equally bad as just picking a random class.
One more thing is that a significant portion the models that where tried with the AttributeSelectedClassifier is that they almost all chose these

\begin{table}[H]
\caption{label{Protein Selection }Most selected protein}
\begin{tabular}{l}
 Protein RefSeq \\
 NP\_008832 \\
 NP\_056289 \\
 NP\_001349 \\
 NP\_001349 \\
 NP\_055719 \\
 NP\_001150 \\
 NP\_079093 \\
 NP\_000959 \\
 NP\_004893 \\
 NP\_004887 \\
 NP\_065901 \\
 NP\_058632 \\
 NP\_002630 \\
 NP\_060947 \\
 NP\_065109 \\
 NP\_848613 \\
 NP\_001035147 \\
 NP\_005639 \\
 NP\_001092102 \\
 NP\_001017
\end{tabular}
\end{table}

These can be of interest for further research after collecting more instance. / samples.

#### FINAL MODEL
So after all this what was the final model ?
It's a Cost sensetive classifiere using Adaboost M1 running a rondomTree with Seed 38.
First off this model is not correct in any way and is not a good One, it is likely overfitted since it uses all 9200 attributes.
Two the cost matrix is wrong since it contains a mistake. Three its accuracy and cost matrix are still not good, but its ROC is one of the best found, although it is till a very bad one see figure 5
It can be found here in the repository`Models/Model-01_27_10.model`
\newline

```{r roc plot, fig.cap = "Plotting of Roc curve per class", out.width = "90%"}
roc.t1 <- read.arff("Analysis//data//ROCplots//T1_roc.arff")
roc.t2 <- read.arff("Analysis//data//ROCplots//T2_roc.arff")
roc.t3 <- read.arff("Analysis//data//ROCplots//T3_roc.arff")
roc.t4 <- read.arff("Analysis//data//ROCplots//T4_roc.arff")

t1plot <- ggplot(data = roc.t1) +
  geom_point(aes(`False Positive Rate`, `True Positive Rate`, colour = Threshold))

t2plot <- ggplot(data = roc.t2) +
  geom_point(aes(`False Positive Rate`, `True Positive Rate`, colour = Threshold))

t3plot <- ggplot(data = roc.t3) +
  geom_point(aes(`False Positive Rate`, `True Positive Rate`, colour = Threshold))

t4plot <- ggplot(data = roc.t4) +
  geom_point(aes(`False Positive Rate`, `True Positive Rate`, colour = Threshold))

ggarrange(t1plot, t2plot, t3plot, t4plot,
          labels = c("T1 ROC", "T2 ROC", "T3 ROC", "T4 ROC"),
          ncol = 2, nrow = 2)
```


It results are follows:
=== Run information ===

Scheme:       weka.classifiers.meta.CostSensitiveClassifier -cost-matrix "[0.0 20.0 20.0 20.0; 1.0 0.0 1.0 1.0; 20.0 20.0 0.0 20.0; 20.0 20.0 1.0 0.0]" -S 38 -W weka.classifiers.meta.AdaBoostM1 -- -P 100 -S 38 -I 10 -W weka.classifiers.trees.RandomTree -- -K 0 -M 1.0 -V 0.001 -S 38 \newline
Relation:     R_data_frame \newline
Instances:    77 \newline
Attributes:   9200 \newline
              [list of attributes omitted] \newline
Test mode:    77-fold cross-validation \newline

=== Classifier model (full training set) === \newline

CostSensitiveClassifier using reweighed training instances \newline

weka.classifiers.meta.AdaBoostM1 -P 100 -S 38 -I 10 -W weka.classifiers.trees.RandomTree -- -K 0 -M 1.0 -V 0.001 -S 38 \newline

Classifier Model \newline
AdaBoostM1: No boosting possible, one classifier used! \newline

Cost Matrix \newline
  0 20 20 20 \newline
  1  0  1  1 \newline
 20 20  0 20 \newline
 20 20  1  0 \newline


Time taken to build model: 0.05 seconds \newline
\newline
=== Summary === \newline

Correctly Classified Instances          37               48.0519 % \newline
Incorrectly Classified Instances        40               51.9481 % \newline
Kappa statistic                         -0.0052 \newline
Total Cost                             116      \newline
Average Cost                             1.5065 \newline
Mean absolute error                      0.2582 \newline
Root mean squared error                  0.5062 \newline
Relative absolute error                 96. \newline
Root relative squared error            138.6 \newline
Total Number of Instances               77     \newline

=== Detailed Accuracy By Class === \newline

\begin{table}[tbh]
\caption{label{Final model accuracy} final model accuracy }
\begin{tabular}{l|lllllllll}
  & TP Rate & FP Rate & Precision & Recall  & F-Measure & MCC   &   ROC Area & PRC Area & Class \\
   & 0,100  &  0,134  &  0,100   &   0,100  &  0,100    &  -0,034   &  0,494  &  0,133  &   T1 \\
   & 0,647  &  0,692  &  0,647   &   0,647  &  0,647    &   -0,045  &  0,535  &  0,709  &   T2 \\
   & 0,273  &  0,167  &  0,214   &   0,273  &  0,240    &  0,096    & 0,594   &  0,177  &   T3 \\
   & 0,000  &  0,028  &  0,000   &   0,000  &  0,000    &   -0,043  & 0,631   &  0,112  &   T4 \\
  Weighted Avg. & 0,481  &  0,502  &  0,472   &   0,481  &  0,476   &   -0,023 &  0,545  &   0,520 &

\end{tabular}
\end{table}
\newpage


\begin{table}[tbh]
\caption{label{cost sensitive Random Tree}Confusion matrix}
\begin{tabular}{llll|l}
 a & b & c & d & <-- classified as \\ \hline
 1 & 7 & 2 & 0 & a = T1 \\ \hline
 8 & 33 & 8 & 2 & b = T2 \\ \hline
 0 & 8 & 3 & 0 & c = T3 \\ \hline
 1 & 3 & 1 & 0 & d = T4
\end{tabular}
\end{table}
\newpage